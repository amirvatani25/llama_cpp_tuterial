{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 363 tensors from ./models/llama-2-13b-chat.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q4_K:  241 tensors\n",
      "llama_model_loader: - type q6_K:   41 tensors\n",
      "llm_load_vocab: special tokens cache size = 259\n",
      "llm_load_vocab: token to piece cache size = 0.1684 MB\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 5120\n",
      "llm_load_print_meta: n_embd_v_gqa     = 5120\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 7.33 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.18 MiB\n",
      "llm_load_tensors:        CPU buffer size =  7500.85 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   400.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  400.00 MiB, K (f16):  200.00 MiB, V (f16):  200.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    85.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1286\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '5120', 'llama.feed_forward_length': '13824', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '40', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '40', 'llama.attention.head_count_kv': '40', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from llama_cpp import Llama\n",
    "model_path = \"./models/llama-2-13b-chat.Q4_K_M.gguf\"\n",
    "model: Llama = Llama(model_path=\"./models/llama-2-13b-chat.Q4_K_M.gguf\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T05:38:44.555777110Z",
     "start_time": "2024-06-06T05:38:36.906984456Z"
    }
   },
   "id": "ec4e609891596627"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': '425 miles from the city of esfahan.\\nThe capital of', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}]\n"
     ]
    }
   ],
   "source": [
    "#want to reduce the veerbosity of the output a bit\n",
    "\n",
    "model.verbose = False\n",
    "\n",
    "#Now that we have a model let's actually do some inference! The\n",
    "# method we use for this is called create_completion(), and by\n",
    "#defualt we only neet to pass it a prompt to complete and it\n",
    "# returns to us a Completion object, which is a TypedDict.\n",
    "from llama_cpp.llama_types import *\n",
    "\n",
    "result: Completion = model.create_completion(prompt=\"the capital of iran is \")\n",
    "#the completion type has a choice key which shows us the list of \n",
    "#responses the LLM genrated, let's take a look\n",
    "print(result[\"choices\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-05T23:28:06.869196506Z",
     "start_time": "2024-06-05T23:28:02.329439947Z"
    }
   },
   "id": "a3beda963dfe77cd"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp=0.0, run=0, result: 1) Mercury, 2) Venus, 3) Earth,\n",
      "temp=0.0, run=1, result: 1) Mercury, 2) Venus, 3) Earth,\n",
      "temp=0.0, run=2, result: 1) Mercury, 2) Venus, 3) Earth,\n",
      "temp=0.5, run=0, result: 1) Mercury, 2) Venus, 3) Earth,\n",
      "temp=0.5, run=1, result: 8 planets, dwarf planets and other smaller bodies. The \n",
      "temp=0.5, run=2, result: 1) Mercury, the closest planet to the sun, with a highly ecc\n",
      "temp=1.0, run=0, result: 1. Mercury: Mercury is closest planet to the Sun. It is\n",
      "temp=1.0, run=1, result: 28 moons, five dwarf planets, asteroids, and\n",
      "temp=1.0, run=2, result: 90,934 and 106,400.\n"
     ]
    }
   ],
   "source": [
    "# We can make the model behave more deterministically by setting its\n",
    "# temperature, which is a parameter from 0 to 1,\n",
    "# where values closer to 0 cause the model to behave more deterministically.\n",
    "# And values closer to 1 cause the model to behave more nondeterministic or creative. \n",
    "######################################\n",
    "# So let's try a few different temperature values, I'm just going to create a list of\n",
    "# floats here, we're going to go from 0 to 0.5 to 1. \n",
    "temps: list[float] = [0.0, 0.5, 1.0]\n",
    "\n",
    "# Now, for each of these temperatures, we're going to do three different completions,\n",
    "# and it really is useful to think of this as a completion.\n",
    "# We're going to give the prompt, it's going to be turned into a sequence of tokens,\n",
    "# and the model underneath is going to just create more tokens for that sequence, and\n",
    "# I'm going to print those all out. \n",
    "\n",
    "prompt: str = \"the planet in the solar system include \"\n",
    "for temp in temps:\n",
    "    for i in range(0 , 3):\n",
    "        result: Completion = model.create_completion(prompt=prompt, temperature=temp)\n",
    "        print(f'temp={temp}, run={i}, result: {result[\"choices\"][0][\"text\"]}')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-05T23:58:23.722048306Z",
     "start_time": "2024-06-05T23:57:41.374282649Z"
    }
   },
   "id": "8d43e17a5ae29c56"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 planets, dwarf planets, asteroids, comets and other smaller bodies.\n",
      "The largest planet in our solar system is Jupiter which has a diameter of more than 142,984 km. The smallest planet is Mercury with a diameter of approximately 4,879km.\n",
      "The distance between the sun and Earth is approximately 93 million miles or 150 million kilometers. It takes Earth approximately 365 days to complete one orbit around the sun.\n",
      "The surface temperature on Venus is about 462 degrees Celsius while the average temperature on Mars is -67 degrees Celsius. Mercury has no atmosphere and can reach temperatures as high as 427 degrees Celsius during the day and as low as -173 degrees Celsius at night.\n",
      "The largest moon in our solar system is Ganymede, which orbits Jupiter and has a diameter of approximately 5,262km. The smallest planet in our solar system is Mercury with no moons.\n",
      "In conclusion, there are numerous interesting facts about the planets in our solar system that make them unique and fascinating. Studying these planets can help us understand more about their formation, composition and potential for supporting life.\n"
     ]
    }
   ],
   "source": [
    "# Generating tokens is slow, and by default the create completion\n",
    "# method limits the number of tokens returned to just 16.\n",
    "# If we change this to -1, we can generate as many tokens as we want,\n",
    "# as many as are available from the model and the context window.\n",
    "# Let's just do one run here,\n",
    "# I'm going to leave the temperature at its default value, which is 0.8. \n",
    "result: Completion = model.create_completion(prompt=prompt, max_tokens=-1)\n",
    "\n",
    "#let's see what we got\n",
    "print(result[\"choices\"][0][\"text\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T00:00:45.450209069Z",
     "start_time": "2024-06-05T23:59:28.921143557Z"
    }
   },
   "id": "83e9db58775f216"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " visiting historical sites, explore natural wonders, and experiencing the local culture.\n",
      "1- Visit Historical Sites: Iran is home to many ancient historical sites that are UNESCO World Heritage Sites, including Persepolis\n",
      ", Pasargadae, and Meidan Emam (Naqsh-e Jahan) in Isfahan. These sites offer a glimpse into the country's rich history and culture.\n",
      "2- Explore Natural\n",
      " Wonders: Iran is home to many natural wonders such as Dasht-e Kavir, a vast desert; Alborz Mountains, the highest mountain range in the country; and the Caspian Sea, the world's largest\n",
      " saltwater lake. These natural wonders offer opportunities for hiking, camping, and water sports.\n",
      "3- Experience Local Culture: Iran is known for its rich cultural heritage, including traditional music, dance, literature,\n",
      " and art. Visitors can experience the local culture by attending a traditional Persian wedding, watching a performance of classical music and dance, or visiting a local bazaar.\n",
      "4- Try Local Cuisine: Iranian cu\n",
      "isine is known for its delicious dishes such as kebabs, stews, and pilafs. Visitors can try local specialties like faludeh (a sweet soup made of vermicelli noodles and herbs\n",
      "), beryani (a rice dish with meat and spices), or gheimeh sabzi (a fried fish with herbs).\n",
      "5- Visit Museums: Iran has many museums that showcase the country\n",
      "'s history, art, and culture. Some of the most famous museums include the National Museum of Iran in Tehran, the Golestan Palace Museum in Tehran, and the Masoumeh Shrine in Mash\n",
      "had.\n",
      "6- Attend Festivals: Iran celebrates many festivals throughout the year, including the Persian New Year (Nowruz) and the Mehregan Festival, which is a celebration of the autumn\n",
      " harvest. These festivals offer visitors a chance to experience local traditions and customs.\n",
      "7- Take a Boat Trip: Iran has many beautiful waterways, including the Karun River in Khuzestan Province and the\n",
      " Caspian Sea. Visitors can take a boat trip to explore these waterways and enjoy the scenic views.\n",
      "8- Go Hiking or Camping: Iran has many hiking and camping opportunities, including the\n",
      " Alborz Mountains, the Zagros Mountains, and the Dasht-e Kavir desert. These areas offer stunning natural beauty and a chance to disconnect from the city and connect with nature.\n",
      "9- Learn Persian\n",
      " Calligraphy: Iran is known for its beautiful calligraphy, which is a unique form of artistic expression. Visitors can take a class to learn about Persian calligraphy and create their own pieces.\n",
      "1\n",
      "0- Take a Hot Air Balloon Ride: Iran offers hot air balloon rides over the country's stunning landscapes, including the Alborz Mountains and the Dasht-e Kavir desert. This\n",
      " is a unique way to experience the country's natural beauty from above.\n",
      "these are just some of the fun things to do in Iran. With its rich history, stunning natural beauty, and vibrant culture, there is something\n",
      " for everyone to enjoy in this fascinating country"
     ]
    }
   ],
   "source": [
    "#############cell for live chat##################\n",
    "\n",
    "\n",
    "# When you create a new Llama CPP object,\n",
    "# the default context length is set to 512 tokens okay, let's do one last demo.\n",
    "# And here I want to show you that we actually don't have to wait for\n",
    "# the whole query to finish, but instead can use the streaming features of\n",
    "# Llama CPP to see the tokens as they're completed. \n",
    "\n",
    "\n",
    "# So I'm going to create a new model, very similar,\n",
    "# I'm setting a nice big context window here. \n",
    "model: Llama = Llama(model_path=\"./models/llama-2-13b-chat.Q4_K_M.gguf\", verbose=False , n_ctx=4096)\n",
    "\n",
    "# If we pass this stream = true parameter to create completion,\n",
    "# we're actually going to get back an iterator and\n",
    "# it's going to iterate over these create completion stream response objects.\n",
    "# Again, all of these objects are actually just typed dictionaries and\n",
    "# this one's similar to the completion type. \n",
    "\n",
    "#So I'm going to set a token count here\n",
    "token_count: int = 0\n",
    "\n",
    "#Then I'm just going to iterate\n",
    "# over all of the tokens that come back in this create completion.\n",
    "for result in model.create_completion(\n",
    "    prompt=\"some fun things to do for vacation in the iran includes \",\n",
    "    max_tokens=-1,\n",
    "    stream=True,\n",
    "):\n",
    "# Importantly here, I've just got this little mod, 50 = 0, that's just so that I\n",
    "# can print new line characters, otherwise everything's going to be on one line. \n",
    "    if token_count % 50 == 0 :\n",
    "        print(\"\")\n",
    "    token_count = token_count +1\n",
    "    print(result[\"choices\"][0][\"text\"], end=\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T00:28:19.710537124Z",
     "start_time": "2024-06-06T00:19:55.922544440Z"
    }
   },
   "id": "6fcbe0b8355883f7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#############cell for live chat##################\n",
    "\n",
    "\n",
    "# When you create a new Llama CPP object,\n",
    "# the default context length is set to 512 tokens okay, let's do one last demo.\n",
    "# And here I want to show you that we actually don't have to wait for\n",
    "# the whole query to finish, but instead can use the streaming features of\n",
    "# Llama CPP to see the tokens as they're completed. \n",
    "\n",
    "\n",
    "# So I'm going to create a new model, very similar,\n",
    "# I'm setting a nice big context window here. \n",
    "model: Llama = Llama(model_path=\"./models/llama-2-13b-chat.Q4_K_M.gguf\", verbose=False , n_ctx=4096)\n",
    "\n",
    "# If we pass this stream = true parameter to create completion,\n",
    "# we're actually going to get back an iterator and\n",
    "# it's going to iterate over these create completion stream response objects.\n",
    "# Again, all of these objects are actually just typed dictionaries and\n",
    "# this one's similar to the completion type. \n",
    "\n",
    "#So I'm going to set a token count here\n",
    "token_count: int = 0\n",
    "\n",
    "#Then I'm just going to iterate\n",
    "# over all of the tokens that come back in this create completion.\n",
    "for result in model.create_completion(\n",
    "    prompt=\"write me in persian\",\n",
    "    max_tokens=-1,\n",
    "    stream=True,\n",
    "):\n",
    "# Importantly here, I've just got this little mod, 50 = 0, that's just so that I\n",
    "# can print new line characters, otherwise everything's going to be on one line. \n",
    "    if token_count % 50 == 0 :\n",
    "        print(\"\")\n",
    "    token_count = token_count +1\n",
    "    print(result[\"choices\"][0][\"text\"], end=\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-06-06T05:57:25.332831950Z"
    }
   },
   "id": "2c2271adcfe61090"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "bd2e22479e6ff641"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
